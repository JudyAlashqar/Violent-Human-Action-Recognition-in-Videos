{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d143cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seqborn as sns\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras import Sequential\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import callbacks\n",
    "from collections import deque\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38c4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define HyperParameters\n",
    "\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224\n",
    "NUM_FEATURES = 1024\n",
    "SEQUENCE_LENGTH = 10\n",
    "DATASET_DIR = \"Real Life Violence Dataset\"\n",
    "path = f'/datasets/{DATASET_DIR}'\n",
    "classes_labels = os.listdir(path)\n",
    "EXAMPLES = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9fa176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing Number of examples in each class\n",
    "\n",
    "for i, vid in enumerate(classes_labels):\n",
    "    class_video_nums = os.listdir(f'{path}/{vid}')\n",
    "    print(\"Class %s has %d examples\" %(vid, len(class_video_nums)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b94eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning Violent=1 and notViolent=0 Label to each example\n",
    "\n",
    "labels = np.zeros(shape=(EXAMPLES))\n",
    "v = 0\n",
    "for ind,val in enumerate(classes_labels):\n",
    "    video_paths = os.listdir(f'{path}/{val}')\n",
    "    print(len(video_paths))\n",
    "    for e, video_path in enumerate(video_paths):\n",
    "        if ( val == 'Violence'):\n",
    "            labels[v] = 1\n",
    "        else:\n",
    "            labels[v] = 0\n",
    "        v = v + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e6088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract and process the frames of a Video example, return ndarray with RGB values of 'SEQUENCE_LENGTH' Frames the Video example\n",
    "\n",
    "def frames_extraction(video_path, method):\n",
    "    frames_list = []\n",
    "    video_reader = cv2.VideoCapture(video_path)\n",
    "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if (video_frames_count < SEQUENCE_LENGTH):\n",
    "        return frames_list\n",
    "    if (method == 'sparse'): #Takes 'SEQUENCE_LENGTH' Frames sparsed along the video\n",
    "        skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n",
    "        for frame_counter in range(SEQUENCE_LENGTH):\n",
    "            video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
    "            ok, frame = video_reader.read() \n",
    "            if not ok:\n",
    "                break\n",
    "            frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_HEIGHT))\n",
    "            frame = frame / 255\n",
    "            frames_list.append(frame)\n",
    "    elif (method == 'sequential'): #Takes First 'SEQUENCE_LENGTH' Frame\n",
    "        for frame_counter in range(SEQUENCE_LENGTH):\n",
    "            ok, frame = video_reader.read() \n",
    "            if not ok:\n",
    "                break\n",
    "            frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_HEIGHT))\n",
    "            frame = frame / 255\n",
    "            frames_list.append(frame)\n",
    "    video_reader.release() \n",
    "    return frames_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa2cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We Get the features in this way if we do NOT use a pre-trained model\n",
    "# return ndarray with RGB values of 'SEQUENCE_LENGTH' Frames the all the video examples in the dataset\n",
    "\n",
    "def get_features():\n",
    "    features = np.zeros(shape=(EXAMPLES,SEQUENCE_LENGTH,IMAGE_HEIGHT,IMAGE_WIDTH,3),dtype=np.float32) #float32 to minimze using RAM\n",
    "    frames = np.zeros(shape=(SEQUENCE_LENGTH,IMAGE_HEIGHT,IMAGE_WIDTH,3),dtype=np.float32)\n",
    "    labels = []\n",
    "    counter = 0\n",
    "\n",
    "    for class_index, class_name in enumerate(classes_labels):\n",
    "        print(class_name)\n",
    "        video_paths = os.listdir(f'{path}/{class_name}')\n",
    "        for e, video_path in enumerate(video_paths):\n",
    "            frames = frames_extraction(f'{path}/{class_name}/{video_path}', 'sparse')\n",
    "            frames = np.array(frames)\n",
    "            if (len(frames) < SEQUENCE_LENGTH):\n",
    "                continue;\n",
    "            features[counter][:] = frames\n",
    "            labels.append(class_index)\n",
    "            counter = counter + 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20676a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We Get the features in this way if we do NOT use a pre-trained model\n",
    "\n",
    "features = get_features()\n",
    "print(features.shape) #should be (2000,10,224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b50a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS cell runs ONLY WHEN WE USE a PRE-TRAINED MODEL\n",
    "# Build Pre-trained CNN model to extract features maps from the data\n",
    "\n",
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.MobileNet(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling= 'avg',\n",
    "        input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.mobilenet.preprocess_input\n",
    "    inputs = keras.Input((IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    extractor = keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "    return extractor\n",
    "\n",
    "extractor = build_feature_extractor()\n",
    "for layer in extractor.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4c1ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS cell runs ONLY WHEN WE USE a PRE-TRAINED MODEL\n",
    "# get The Features from the defined pre-trained model to use it directly in a fine-tuned model.\n",
    "# Keeping only the features extracted by the pre-trained model reserves using unnecessary excessive RAM to store all the data\n",
    "\n",
    "def prepare_all_videos():\n",
    "    features = np.zeros(shape=(EXAMPLES,SEQUENCE_LENGTH,NUM_FEATURES),dtype=np.float32) #NUM_FEATURES is the number of features returned by the pretrained model\n",
    "    frames = []\n",
    "    pred_video_frames = np.zeros(shape=(SEQUENCE_LENGTH,NUM_FEATURES),dtype=np.float32)\n",
    "    counter = 0\n",
    "    for class_index, class_name in enumerate(classes_labels):\n",
    "        print(class_name)\n",
    "        video_paths = os.listdir(f'/gdrive/MyDrive/{DATASET_DIR}/{class_name}')\n",
    "        for e, video_path in enumerate(video_paths):\n",
    "            print(e)\n",
    "            frames = frames_extraction(f'/gdrive/MyDrive/{DATASET_DIR}/{class_name}/{video_path}', 'sparse')\n",
    "            if (len(frames) < SEQUENCE_LENGTH):\n",
    "                continue;\n",
    "            for idx, f in enumerate(frames):\n",
    "                pred_video_frames[idx,:] = extractor.predict(np.array(f).reshape(1,IMAGE_HEIGHT,IMAGE_WIDTH,3))\n",
    "            features[counter][:] = pred_video_frames.squeeze()\n",
    "            counter = counter + 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd6acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS cell runs ONLY WHEN WE USE a PRE-TRAINED MODEL\n",
    "\n",
    "features = prepare_all_videos()\n",
    "print(features.shape) #should be (2000,10,1024) because 1024 is the number of features returned by MobileNet\n",
    "#if we use Global Average Pooling in the final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e826d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test Split Method: 80% Training, 20% Testing\n",
    "# used in the first two models Only (No pre-trained models included) because it was hard to use K Fold Cross Validation with them\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, stratify = labels, test_size=0.2,shuffle= True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333be12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation method with 5 splits\n",
    "# used in all the models except the first two for better evaluation\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle= True, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dad582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Model - CNN & LSTM (Without pre-trained CNN model)\n",
    "\n",
    "def cnn_lstm_model():\n",
    "    cnn = keras.Sequential([\n",
    "      keras.layers.Conv2D(64, (3, 3), padding='same',activation = 'relu',input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)),\n",
    "      keras.layers.BatchNormalization(momentum = 0.9),\n",
    "      keras.layers.AveragePooling2D(pool_size = (2,2), strides=2),\n",
    "      keras.layers.Conv2D(128, (3, 3), padding='same',activation = 'relu'),\n",
    "      keras.layers.BatchNormalization(momentum = 0.9),\n",
    "      keras.layers.AveragePooling2D( pool_size = (2,2), strides=2),\n",
    "      keras.layers.Conv2D(256, (3, 3), padding='same',activation = 'relu'),\n",
    "      keras.layers.BatchNormalization(momentum = 0.9),\n",
    "      keras.layers.GlobalAveragePooling2D(),\n",
    "  ])\n",
    "    cnn_lstm_model= keras.Sequential([\n",
    "      keras.layers.TimeDistributed(cnn, input_shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH,3)),\n",
    "      keras.layers.LSTM(32),\n",
    "      keras.layers.Dense(512, activation= 'relu'),\n",
    "      keras.layers.Dropout(0.5),\n",
    "      keras.layers.Dense(128, activation= 'relu'),\n",
    "      keras.layers.Dropout(0.3),\n",
    "      keras.layers.Dense(64, activation= 'relu'),\n",
    "      keras.layers.Dropout(0.2),\n",
    "      keras.layers.Dense(2, activation= 'sigmoid')\n",
    "  ])\n",
    "    cnn.summary()\n",
    "    cnn_lstm_model.summary()\n",
    "    return cnn_lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a476cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Model - 3D CNN (Without pre-trained CNN model)\n",
    "\n",
    "def create_3dcnn_model():\n",
    "    model = Sequential([\n",
    "      keras.layers.Conv3D(64, (3, 3, 3), padding='same',activation = 'relu',input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)),\n",
    "      keras.layers.BatchNormalization(momentum = 0.9),\n",
    "      keras.layers.AveragePooling3D(pool_size = (1,2,2), strides=(1,2,2)),\n",
    "      keras.layers.Conv3D(128, (3, 3, 3), padding='same',activation = 'relu'),\n",
    "      keras.layers.BatchNormalization(momentum = 0.9),\n",
    "      keras.layers.AveragePooling3D( pool_size = (2,2,2), strides=2),\n",
    "      keras.layers.Conv3D(256, (3, 3, 3), padding='same',activation = 'relu'),\n",
    "      keras.layers.BatchNormalization(momentum = 0.9),\n",
    "      keras.layers.GlobalAveragePooling3D(),\n",
    "      keras.layers.Dense(512, activation = 'relu'),\n",
    "      keras.layers.Dropout(0.5),\n",
    "      keras.layers.Dense(128, activation = 'relu'),\n",
    "      keras.layers.Dropout(0.3),\n",
    "      keras.layers.Dense(64, activation = 'relu'),\n",
    "      keras.layers.Dropout(0.2),\n",
    "      keras.layers.Dense(2, activation=\"softmax\")\n",
    "      ])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af79e119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Example for the first model, the second model has been trained the same way but with epochs = 15\n",
    "\n",
    "model = cnn_lstm_model()\n",
    "earlyStopping = EarlyStopping(monitor = 'val_loss', patience = 20, mode = 'min', min_delta=0.001, restore_best_weights = True)\n",
    "adam = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "reduce_lr_loss = callbacks.ReduceLROnPlateau(monitor='val_loss',patience=5, verbose=1,factor=0.5,min_lr=0.0000001)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "history = model.fit(features_train, to_categorical(labels_train),\n",
    "      batch_size = 20,\n",
    "      epochs=100,\n",
    "      verbose=1,\n",
    "      validation_data=(features_test,to_categorical(labels_test)),\n",
    "      callbacks=[earlyStopping, reduce_lr_loss]\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a46c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning curves have been plotted for the first and second model in this way\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "#Plotting Loss\n",
    "plt.plot(history_df.loc[:, ['loss']], label='loss')\n",
    "plt.plot(history_df.loc[:, ['val_loss']], label='Validation loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Training Loss and Validation Loss for 3D CNN model\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Plotting Accuracy\n",
    "plt.plot(history_df.loc[:, ['accuracy']], label='accuracy')\n",
    "plt.plot(history_df.loc[:, ['val_accuracy']], label='Validation accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Training Accuracy and Validation Accuracy for 3D CNN model\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32cf69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the Confusion matrix\n",
    "# label = 1 then Violent, label = 0 then non Violent\n",
    "\n",
    "pred = model.predict(features_test)\n",
    "yprd = pred > 0.5\n",
    "ypredicted = []\n",
    "for zero,one in yprd:\n",
    "    if zero == True:\n",
    "        ypredicted.append(0)\n",
    "    else:\n",
    "        ypredicted.append(1) \n",
    "y = []\n",
    "for zero,one in to_categorical(labels_test):\n",
    "    if zero == True:\n",
    "        y.append(0)\n",
    "    else:\n",
    "        y.append(1) \n",
    "\n",
    "confusion = confusion_matrix(y,ypredicted)\n",
    "confusion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3f2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the Confusion matrix\n",
    "\n",
    "def print_confusion_matrix(confusion_matrix, class_names):\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure()\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return fig\n",
    "\n",
    "print_confusion_matrix(confusion, [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de03329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third, Fourth and Fifth Model (Pre-trained CNN models used with BiLSTM Layer)\n",
    "# Each of VGG16 Extracted Features, ResNet50 Extracted Features and MobileNet Extracted Features have been passed\n",
    "# as input to this pre-trained model, as a result we got three models \n",
    "\n",
    "def PreTrained_BiLSTM():\n",
    "    PreTrained_BiLSTM = keras.Sequential([\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(80),input_shape=(SEQUENCE_LENGTH, NUM_FEATURES)),\n",
    "    keras.layers.Dense(512, activation = 'relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(128, activation = 'relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(64, activation = 'relu'),\n",
    "    keras.layers.Dense(len(classes_labels), activation=\"softmax\")\n",
    "    ])\n",
    "    return PreTrained_BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf206fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sixth, Seventh and eighth Model (Pre-trained CNN models used with LSTM Layer)\n",
    "# Each of VGG16 Extracted Features, ResNet50 Extracted Features and MobileNet Extracted Features have been passed\n",
    "# as input to this pre-trained model, as a result we got three models \n",
    "\n",
    "def PreTrained_LSTM():\n",
    "    PreTrained_LSTM = keras.Sequential([\n",
    "    keras.layers.LSTM(80,input_shape=(SEQUENCE_LENGTH, NUM_FEATURES)),\n",
    "    keras.layers.Dense(512, activation = 'relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(128, activation = 'relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(64, activation = 'relu'),\n",
    "    keras.layers.Dense(len(classes_labels), activation=\"softmax\")\n",
    "    ])\n",
    "    return PreTrained_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2181314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ninth ,tenth and eleventh Model (Pre-trained CNN models used with GRU Layer)\n",
    "# Each of VGG16 Extracted Features, ResNet50 Extracted Features and MobileNet Extracted Features have been passed\n",
    "# as input to this pre-trained model, as a result we got three models \n",
    "\n",
    "def PreTrained_GRU():\n",
    "    PreTrained_GRU = keras.Sequential([\n",
    "    keras.layers.GRU(80,input_shape=(SEQUENCE_LENGTH, NUM_FEATURES)),\n",
    "    keras.layers.Dense(512, activation = 'relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(128, activation = 'relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(64, activation = 'relu'),\n",
    "    keras.layers.Dense(len(classes_labels), activation=\"softmax\")\n",
    "    ])\n",
    "    return PreTrained_GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0d74de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twelveth and Thirteenth Model\n",
    "#Pre-trained CNN ResNet50  used with GRU returns hidden states\n",
    "#Pre-trained CNN MobileNet used with GRU returns hidden states\n",
    "\n",
    "def model_GRU_hidden():\n",
    "    model_GRU_hidden = keras.Sequential([\n",
    "    keras.layers.GRU(80,input_shape=(SEQUENCE_LENGTH, NUM_FEATURES),return_sequences=True),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dense(512, activation = 'relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(128, activation = 'relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(64, activation = 'relu'),\n",
    "    keras.layers.Dense(len(classes_labels), activation=\"softmax\")\n",
    "    ])\n",
    "    return model_GRU_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb6edfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Example for the models from 3 to 13 with 5 folds cross validation method\n",
    "# We calculate the accuracy, f1-score and standard deviation for each fold\n",
    "\n",
    "accuracy = []\n",
    "train_accuracy = []\n",
    "f1Score = []\n",
    "skf = StratifiedKFold(n_splits=10, shuffle= True, random_state = 1)\n",
    "i = 1\n",
    "for train_index, test_index in skf.split(features, labels):\n",
    "    earlyStopping = EarlyStopping(monitor = 'val_loss', patience = 20, mode = 'min', min_delta=0.001, restore_best_weights = True)\n",
    "    adam = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "    reduce_lr_loss = callbacks.ReduceLROnPlateau(monitor='val_loss',patience=5, verbose=1,factor=0.5,min_lr=0.0000001)\n",
    "    print('Fold ',i)\n",
    "    features_train, features_test = features[train_index][:], features[test_index][:]\n",
    "    labels_train, labels_test = labels[train_index][:], labels[test_index][:]\n",
    "    model = MobileNet_BiLSTM()\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "    history = model.fit(features_train, to_categorical(labels_train),\n",
    "          batch_size = 20,\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          validation_data=(features_test, to_categorical(labels_test)),\n",
    "          callbacks=[earlyStopping, reduce_lr_loss]\n",
    "          )\n",
    "    pred = model.predict(features_test)\n",
    "    pred = pred > 0.5\n",
    "    ypredic = []\n",
    "    counter = 0\n",
    "    \n",
    "    for zero,one in pred:\n",
    "        if zero == True:\n",
    "            ypredic.append(0)\n",
    "        else:\n",
    "            ypredic.append(1)\n",
    "    print(f1_score(labels_test, ypredic, average='macro'))\n",
    "    print('-------------')\n",
    "    print(model.evaluate(features_test,to_categorical(labels_test)))\n",
    "    print(model.evaluate(features_train,to_categorical(labels_train)))\n",
    "    accuracy.append(model.evaluate(features_test,to_categorical(labels_test)))\n",
    "    train_accuracy.append(model.evaluate(features_train,to_categorical(labels_train)))\n",
    "    f1Score.append(f1_score(labels_test, ypredic, average='macro'))\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d18220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the results of the training process in the previos cell \n",
    "\n",
    "accuracy = np.array(accuracy)\n",
    "train_accuracy = np.array(train_accuracy)\n",
    "print(\"Mean Accuracy Train: \\t\", statistics.mean(train_accuracy[:, 1]))\n",
    "print(\"Mean Accuracy: \\t\\t\", statistics.mean(accuracy[:, 1]))\n",
    "print(\"Mean Loss Train: \\t\", statistics.mean(train_accuracy[:, 0]))\n",
    "print(\"Mean Loss: \\t\\t\", statistics.mean(accuracy[:, 0]))\n",
    "print(\"Mean f1-score: \\t\\t\", statistics.mean(f1Score))\n",
    "print(\"Stdev Accuracy: \\t\", statistics.stdev(accuracy[:, 1]))\n",
    "print(\"Stdev f1-score: \\t\",statistics.stdev(f1Score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6032e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the previous results\n",
    "\n",
    "Folds = range(1,11)\n",
    "\n",
    "# Plotting Accuracy among 10 folds\n",
    "plt.xlim(0,10)\n",
    "plt.plot(Folds,accuracy[:, 1], label='Accuracy')\n",
    "plt.axis('on')\n",
    "plt.xlabel(\"Fold\")\n",
    "plt.title(\"Accuracy among 10 folds for VGG16 and BiLSTM\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting f1-score among 10 folds\n",
    "plt.xlim(0,10)\n",
    "plt.plot(Folds,f1Score, label='f1-score')\n",
    "plt.axis('on')\n",
    "plt.xlabel(\"Fold\")\n",
    "plt.title(\"f1-score among 10 folds for VGG16 and BiLSTM\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting Training and Validation loss among 10 folds\n",
    "plt.xlim(0,10)\n",
    "plt.plot(Folds,train_accuracy[:, 0], label='loss')\n",
    "plt.plot(Folds,accuracy[:, 0], label='val_loss')\n",
    "plt.axis('on')\n",
    "plt.xlabel(\"Fold\")\n",
    "plt.title(\"Training and Validation loss among 10 folds for VGG16 and BiLSTM\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb32e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('C:\\\\Users\\\\ASUS\\\\MyModel.hdf5')\n",
    "model = load_model('C:\\\\Users\\\\ASUS\\\\MyModel.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d253f114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video segmentaion with Sliding Window Approach\n",
    "# Window Size = 10, Stride = 1 or 10 (determined by 'mode' Variable)\n",
    "# extractor is the CNN pre-trained we want to use, model is what follows it\n",
    "\n",
    "def predTest(video_file_path,output_file_path,file_name,mode, SEQUENCE_LENGTH, IMAGE_SIZE, NUM_FEATURES,extractor,model):\n",
    "    video_reader = cv2.VideoCapture(video_file_path)\n",
    "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), \n",
    "                                 int(video_reader.get(cv2.CAP_PROP_FPS)), (original_video_width, original_video_height))\n",
    "    framing_rate = int(video_reader.get(cv2.CAP_PROP_FPS))\n",
    "    frames_queue = deque(maxlen = SEQUENCE_LENGTH)\n",
    "    predicted_class_name = ''\n",
    "    m = ''\n",
    "    counter = 0\n",
    "    total = 0\n",
    "    frame_num = 0\n",
    "    data = {'framing rate': str(framing_rate), 'frames': {'1':'0',  '2':'0','3':'0', '4':'0', '5':'0', '6':'0', '7':'0', '8':'0', '9':'0'}}\n",
    "    while video_reader.isOpened():\n",
    "        ok, frame = video_reader.read() \n",
    "\n",
    "        if not ok:\n",
    "            break\n",
    "        frame_num = frame_num + 1\n",
    "        resized_frame = cv2.resize(frame, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        frames_queue.append(extractor.predict(np.array(resized_frame).reshape(1,IMAGE_SIZE,IMAGE_SIZE,3)))\n",
    "        predicted_labels_probabilities = np.zeros(shape=(1,2))\n",
    "        if len(frames_queue) == SEQUENCE_LENGTH:\n",
    "            predicted_labels_probabilities = model.predict(np.array(frames_queue).reshape(1,SEQUENCE_LENGTH,NUM_FEATURES))\n",
    "            total = total + 1\n",
    "            m = str(np.amax(predicted_labels_probabilities))\n",
    "            pred = predicted_labels_probabilities > 0.5\n",
    "            ypredic = ''\n",
    "            predicted_label = ''\n",
    "            for zero,one in pred:\n",
    "                if zero == True:\n",
    "                    ypredic=  0\n",
    "                else:\n",
    "                    ypredic =1 \n",
    "            if (ypredic == 0):\n",
    "                predicted_label = 'NonViolence'\n",
    "                data['frames'].update({str(frame_num):str(1-np.amax(predicted_labels_probabilities))})\n",
    "            else:\n",
    "                predicted_label = 'Violence'\n",
    "                counter = counter + 1\n",
    "                data['frames'].update({str(frame_num):m})\n",
    "            predicted_class_name = predicted_label\n",
    "            if (mode == '10 Steps'):\n",
    "                frames_queue.clear()\n",
    "        cv2.putText(frame, predicted_class_name , (100, 25), cv2.FONT_HERSHEY_PLAIN  , 2, (255, 255, 255), 2)\n",
    "        cv2.putText(frame, m , (100, 50), cv2.FONT_HERSHEY_PLAIN  , 2, (255, 255, 255), 2)\n",
    "        video_writer.write(frame)\n",
    "        \n",
    "    video_reader.release()\n",
    "    video_writer.release()\n",
    "    with open('static/json/'+file_name[:len(file_name)-4]+'.json', 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    return (counter > 1), counter/total, framing_rate, video_frames_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34229f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the video 'demo.mp4' located in video_file_path is saved with preditions to output_file_path\n",
    "\n",
    "extractor = load_model('C:\\\\Users\\\\ASUS\\\\ResNet50.hdf5')\n",
    "model = load_model('C:\\\\Users\\\\ASUS\\\\ResNet50Improved.hdf5')\n",
    "video_file_path = 'C:\\\\Users\\\\ASUS\\\\demo.mp4'\n",
    "output_file_path = 'C:\\\\Users\\\\ASUS\\\\demos\\\\ResultDemo.mp4'\n",
    "file_name = 'demo.mp4'\n",
    "mode = 'High'   # Stride = 1\n",
    "mode = 'Normal' #Stride = 10\n",
    "\n",
    "predTest(video_file_path ,output_file_path, file_name, mode, SEQUENCE_LENGTH, IMAGE_SIZE, NUM_FEATURES,extractor,model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
